{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pymongo \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2434424, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('class_29.pk')\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'mall_goods_name':'goods_name'})\n",
    "# print ('class: ',df['depth_4'].nunique())\n",
    "# df['depth_4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import sentencepiece as spm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import plot_model\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Activation, Dense, Embedding, Flatten, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Reshape, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[['goods_name','depth_4']]\n",
    "x.to_csv(r'cate.txt', header=None, index=None, sep='\\t')\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=cate.txt --model_prefix=cate --vocab_size=20000 --model_type=unigram') # --model_type=unigram (default), bpe, char, or word\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('cate.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spm(lst):\n",
    "    def clean_text(text):\n",
    "        return re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text)\n",
    "    def clean_num(text):\n",
    "        return re.sub('\\d+', '', text)\n",
    "    def _del(text):\n",
    "        return text.replace('▁','')\n",
    "    a = [clean_text(x) for x in lst] \n",
    "    b = [clean_num(x) for x in a] \n",
    "    c = [_del(x) for x in b]\n",
    "    d = [x for x in c if len(x) != 0]\n",
    "    e = ['즉석죽' if x=='죽' else x for x in d]\n",
    "    f = ['껌껌' if x=='껌' else x for x in e]\n",
    "    g = ['Tea' if x=='티' else x for x in f]\n",
    "    h = [x for x in g if len(x) != 1]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['goods_name'] = df['goods_name'].apply(sp.encode_as_pieces).apply(clean_spm).apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 13928\n"
     ]
    }
   ],
   "source": [
    "sentences = df['goods_name'].drop_duplicates().apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "embedding_dim = 200\n",
    "model = Word2Vec(sentences, size = embedding_dim, window = 3, min_count = 3, workers = 16)\n",
    "\n",
    "word_vectors = model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]\n",
    "print ('Vocab Size:',len(model.wv.vocab))\n",
    "\n",
    "# print (word_vectors.similarity(w1 = '즉석밥', w2 = '햇반'))\n",
    "# print (model.wv.most_similar('햇반')[:5])\n",
    "\n",
    "filename = 'cate_w2v.txt'\n",
    "model.wv.save_word2vec_format(filename, binary = False)\n",
    " \n",
    "embedding_index = {}\n",
    "f = open(os.path.join('','cate_w2v.txt'), encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embedding_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['goods_name']]\n",
    "y = df['depth_4']\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 2020)\n",
    "X_train, X_test = X_train['goods_name'], X_test['goods_name']\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen = max_len) #  padding='post'\n",
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13881\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print (num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1971882 samples, validate on 219099 samples\n",
      "Epoch 1/1\n",
      "1971882/1971882 [==============================] - 509s 258us/step - loss: 0.1704 - acc: 0.9545 - val_loss: 0.0886 - val_acc: 0.9781\n",
      "243443/243443 [==============================] - 9s 35us/step\n",
      "Loss: 0.091 | Accuracy: 0.978\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     67038\n",
      "           1       0.97      0.97      0.97      6182\n",
      "           2       0.97      0.98      0.97      4303\n",
      "           3       0.97      0.97      0.97     12783\n",
      "           4       0.99      0.99      0.99      2878\n",
      "           5       0.98      0.97      0.97     21725\n",
      "           6       0.97      0.97      0.97      9677\n",
      "           7       0.96      0.93      0.95      1532\n",
      "           8       0.98      0.98      0.98      2346\n",
      "           9       0.98      0.99      0.98      8022\n",
      "          10       0.95      0.91      0.93      1418\n",
      "          11       0.97      0.95      0.96       760\n",
      "          12       0.99      0.97      0.98      1551\n",
      "          13       0.97      0.97      0.97      9278\n",
      "          14       0.99      0.96      0.97      1894\n",
      "          15       0.97      0.95      0.96      1583\n",
      "          16       0.99      0.98      0.98      3730\n",
      "          17       0.90      0.88      0.89       462\n",
      "          18       0.97      0.97      0.97      8752\n",
      "          19       0.95      0.96      0.96      2990\n",
      "          20       0.98      0.98      0.98     29099\n",
      "          21       0.99      0.97      0.98      1423\n",
      "          22       0.99      0.97      0.98     20197\n",
      "          23       0.91      0.94      0.93      2941\n",
      "          24       0.95      0.96      0.96       904\n",
      "          25       0.99      0.99      0.99      9736\n",
      "          26       0.99      0.97      0.98      3076\n",
      "          27       0.97      0.97      0.97      5126\n",
      "          28       0.96      0.94      0.95      2037\n",
      "\n",
      "    accuracy                           0.98    243443\n",
      "   macro avg       0.97      0.96      0.97    243443\n",
      "weighted avg       0.98      0.98      0.98    243443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            embedding_dim, \n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = max_len)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((max_len, embedding_dim, 1), input_shape = (max_len, embedding_dim)))\n",
    "# print (model.output_shape)\n",
    "model.add(Conv2D(filters = 32, kernel_size = (4, embedding_dim), strides = (2,2), padding = 'valid'))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(29, activation='softmax'))\n",
    "# print (model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['acc'])\n",
    "history = model.fit(x = X_train, y = y_train, batch_size = 128, epochs = 1, verbose = 1, validation_split = 0.1)\n",
    "# evaluate\n",
    "acc = model.evaluate(X_test,y_test)\n",
    "print('Loss: {:0.3f} | Accuracy: {:0.3f}'.format(acc[0],acc[1])) \n",
    "print ('=' * 50)\n",
    "pred = model.predict(X_test)\n",
    "pred_bool = np.argmax(pred,1)\n",
    "y_test_bool = np.argmax(y_test,1)\n",
    "print(classification_report(y_test_bool, pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 쉐프 쉐푸드 명란 오일 파스타 즉석식품 냉동식품\n",
      "['면류'] [0.9845915]\n"
     ]
    }
   ],
   "source": [
    "text = '쉐프드 쉐푸드 명란오일파스타 285g 6종 즉석식품 냉동식품'\n",
    "pre = ' '.join(clean_spm(sp.encode_as_pieces(text)))\n",
    "print ('pre:',pre)\n",
    "t = sequence.pad_sequences(tokenizer.texts_to_sequences([pre]), maxlen = max_len)\n",
    "Preds = model.predict(t)\n",
    "\n",
    "p = [np.argmax(x) for x in Preds]\n",
    "prob = [np.max(x) for x in Preds]\n",
    "\n",
    "pred = label_encoder.inverse_transform(p)\n",
    "print(pred, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cate_food_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cate_food_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
